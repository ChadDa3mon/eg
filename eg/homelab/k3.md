# K3s

Show which node owns the VIP lease
    sudo k3s kubectl -n kube-system describe lease plndr-cp-lock | egrep 'Holder Identity|Renew Time'

Show all cluster nodes and their roles
    sudo k3s kubectl get nodes -o wide

Check core system pods (control plane, DNS, etc.)
    sudo k3s kubectl -n kube-system get pods -o wide

Show which node owns the API VIP lease
    sudo k3s kubectl -n kube-system describe lease plndr-cp-lock | egrep 'Holder Identity|Renew Time'

Find which node currently has the API VIP (123.123.3.34)
    for ip in 123.123.3.31 123.123.3.32 123.123.3.33; do echo "=== $ip ==="; ssh chadd@$ip "ip -4 addr show dev eth0 | grep 123.123.3.34 || echo 'VIP not here'"; done

Test Kubernetes API through the VIP
    curl -k https://123.123.3.34:6443/healthz

Use kubectl via the API VIP instead of localhost
    sudo KUBECONFIG=/etc/rancher/k3s/k3s-vip.yaml kubectl get nodes -o wide

Check Traefik LoadBalancer service and external IP
    sudo k3s kubectl -n kube-system get svc traefik -o wide

Test HTTP access through the apps VIP (123.123.3.30)
    curl -v http://123.123.3.30/

Test Ingress routing with a specific Host header
    curl -v -H 'Host: docker.mylab.com' http://123.123.3.30/

List MetalLB pods and which nodes they run on
    sudo k3s kubectl -n metallb-system get pods -o wide

Show configured MetalLB IP pools
    sudo k3s kubectl -n metallb-system get ipaddresspools -o wide

Show MetalLB L2 advertisements
    sudo k3s kubectl -n metallb-system get l2advertisements -o wide

Check MetalLB speaker logs for ARP announcements/issues
    sudo k3s kubectl -n metallb-system logs -l component=speaker --tail=100

Check Traefik logs for Ingress routing issues
    sudo k3s kubectl -n kube-system logs deploy/traefik --tail=100

List all Ingress resources in the cluster
    sudo k3s kubectl get ingress -A -o wide

Describe a specific Ingress (routing rules + backend)
    sudo k3s kubectl -n demo describe ingress whoami

Check endpoints behind a Service (are pods actually attached?)
    sudo k3s kubectl -n demo get endpoints whoami -o wide

Watch pods schedule across nodes in real time
    sudo k3s kubectl get pods -A -o wide -w

Drain a node for maintenance (evict workloads safely)
    sudo k3s kubectl drain k3s-2 --ignore-daemonsets --delete-emptydir-data

Uncordon a node after maintenance
    sudo k3s kubectl uncordon k3s-2

Restart k3s service on a node (simulate failure)
    ssh chadd@123.123.3.32 "sudo systemctl restart k3s"

Check k3s service status on a node
    ssh chadd@123.123.3.31 "sudo systemctl status k3s --no-pager -l"

# Debug Networking

Show node IPs and pod CIDRs
    sudo k3s kubectl get nodes -o wide

List all cluster services and their cluster IPs
    sudo k3s kubectl get svc -A -o wide

Check which pod IPs exist on this node
    ip route | grep 10.42.

Show flannel interface and pod network on this node
    ip -4 addr show flannel.1

See which node is advertising the MetalLB VIP (123.123.3.30) via ARP
    ip neigh show | grep 123.123.3.30

Test HTTP connectivity to the Traefik LoadBalancer VIP
    curl -v http://123.123.3.30/

Test routing to an Ingress host manually
    curl -v -H 'Host: docker.mylab.com' http://123.123.3.30/

Check MetalLB speaker logs for ARP or announcement issues
    sudo k3s kubectl -n metallb-system logs -l component=speaker --tail=100

Check kube-vip logs for API VIP issues
    sudo k3s kubectl -n kube-system logs -l app=kube-vip -c kube-vip --tail=100

Show which node currently owns the API VIP lease
    sudo k3s kubectl -n kube-system describe lease plndr-cp-lock | egrep 'Holder Identity|Renew Time'

Test Kubernetes API over the VIP
    curl -k https://123.123.3.34:6443/version

Trace route to a pod IP from a node
    traceroute 10.42.1.2

Check open ports listening on a node
    sudo ss -lntp

# Deploying Apps

Create a new namespace
    sudo k3s kubectl create ns myapp

Deploy a simple test app
    sudo k3s kubectl -n myapp create deployment web --image=nginx --replicas=2

Expose a deployment as a ClusterIP service
    sudo k3s kubectl -n myapp expose deployment web --port 80

Expose a service directly via LoadBalancer (MetalLB)
    sudo k3s kubectl -n myapp patch svc web -p '{"spec":{"type":"LoadBalancer","loadBalancerIP":"123.123.3.30"}}'

Create an Ingress to route a hostname to a service
    sudo k3s kubectl apply -f ingress.yaml

List all Ingress rules in the cluster
    sudo k3s kubectl get ingress -A -o wide

Describe an Ingress to see routing details
    sudo k3s kubectl -n myapp describe ingress web

Check endpoints behind a service (are pods attached?)
    sudo k3s kubectl -n myapp get endpoints web -o wide

Tail logs from all pods in a deployment
    sudo k3s kubectl -n myapp logs -l app=web --tail=100 -f

Port-forward a service to your local machine for testing
    sudo k3s kubectl -n myapp port-forward svc/web 8080:80

Scale a deployment up or down
    sudo k3s kubectl -n myapp scale deployment web --replicas=4

Delete a test app and all its resources
    sudo k3s kubectl delete ns myapp

# Cluster Maintenance & Failure Testing

Show overall cluster health at a glance
    sudo k3s kubectl get nodes -o wide && sudo k3s kubectl -n kube-system get pods -o wide | head -n 40

Show pods that are not Running across all namespaces
    sudo k3s kubectl get pods -A | egrep -v 'Running|Completed' || true

Watch scheduling and restarts in real time
.split("\n")
    sudo k3s kubectl get pods -A -o wide -w

Show top resource usage (requires metrics-server)
    sudo k3s kubectl top nodes && sudo k3s kubectl top pods -A

Drain a node for maintenance (evict workloads safely)
    sudo k3s kubectl drain k3s-2 --ignore-daemonsets --delete-emptydir-data

Uncordon a node after maintenance
    sudo k3s kubectl uncordon k3s-2

Cordon a node (prevent new pods from scheduling there)
    sudo k3s kubectl cordon k3s-2

Simulate a node failure (stop k3s on a node)
    ssh chadd@123.123.3.32 "sudo systemctl stop k3s"

Bring a node back (start k3s on a node)
    ssh chadd@123.123.3.32 "sudo systemctl start k3s"

Restart k3s on a node (simulates a quick control-plane bounce)
    ssh chadd@123.123.3.32 "sudo systemctl restart k3s"

Check k3s service status on a node
    ssh chadd@123.123.3.32 "sudo systemctl status k3s --no-pager -l"

Tail k3s logs on a node (most recent 200 lines)
    ssh chadd@123.123.3.32 "sudo journalctl -u k3s -n 200 --no-pager"

Confirm API VIP failover (watch lease holder change)
    watch -n1 "sudo k3s kubectl -n kube-system describe lease plndr-cp-lock | egrep 'Holder Identity|Renew Time'"

Confirm apps VIP still serves traffic during a node outage
    curl -v -H 'Host: docker.mylab.com' http://123.123.3.30/

Check which node currently advertises the MetalLB VIP (ARP neighbor)
    ip neigh show | grep 123.123.3.30

Rollout restart a deployment (safe rolling restart)
    sudo k3s kubectl -n demo rollout restart deployment/whoami

Watch rollout status until complete
    sudo k3s kubectl -n demo rollout status deployment/whoami

Rollback a deployment to previous revision
    sudo k3s kubectl -n demo rollout undo deployment/whoami

Force-delete a stuck pod
    sudo k3s kubectl -n demo delete pod <pod-name> --grace-period=0 --force

Cleanly remove a node from the cluster (delete the Node object)
    sudo k3s kubectl delete node k3s-3
